{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zw0UM_wxLx4x",
        "QzBw0Lc8L4kJ",
        "i98ACMg549uQ",
        "voi7i3JGMIFW",
        "gQumSMveip2Q",
        "v0Q_l97BMSIQ",
        "m7mAFjMApujD",
        "KMkCDia2p4a0",
        "xj2DCnMFqIn8",
        "61HMqWIwIdd6",
        "kUcTBwfbvhco"
      ],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
        "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Deep Learning Project: Hybrid CNN-Vit Models for Land Use Land Cover Classification: Half the dataset"
      ],
      "metadata": {
        "id": "3EVs8cskClMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "import timm\n",
        "import yaml\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "YaV_Ub56C3Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive to load the data\n",
        "drive.mount('/content/drive')\n",
        "!ls\n",
        "%cd /content/drive/My Drive/EuroSAT_RGB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZjCieXSbUo3",
        "outputId": "ae25687c-5408-4a37-bedc-ad74c65705e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  sample_data\n",
            "/content/drive/My Drive/EuroSAT_RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing and preprocessing half the datasat"
      ],
      "metadata": {
        "id": "Zw0UM_wxLx4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists for each class\n",
        "forest = []\n",
        "for image in os.listdir(\"Forest\"):\n",
        "    img = cv2.imread(\"Forest/\" + image)\n",
        "    forest.append((0, img))\n",
        "\n",
        "river = []\n",
        "for image in os.listdir(\"River\"):\n",
        "    img = cv2.imread(\"River/\" + image)\n",
        "    river.append((1, img))\n",
        "\n",
        "annualcrop = []\n",
        "for image in os.listdir(\"AnnualCrop\"):\n",
        "    img = cv2.imread(\"AnnualCrop/\" + image)\n",
        "    annualcrop.append((2, img))\n",
        "\n",
        "herbaceousvegetation = []\n",
        "for image in os.listdir(\"HerbaceousVegetation\"):\n",
        "    img = cv2.imread(\"HerbaceousVegetation/\" + image)\n",
        "    herbaceousvegetation.append((3, img))\n",
        "\n",
        "highway = []\n",
        "for image in os.listdir(\"Highway\"):\n",
        "    img = cv2.imread(\"Highway/\" + image)\n",
        "    highway.append((4, img\t))\n",
        "\n",
        "industrial = []\n",
        "for image in os.listdir(\"Industrial\"):\n",
        "    img = cv2.imread(\"Industrial/\" + image)\n",
        "    industrial.append((5, img))\n",
        "\n",
        "pasture = []\n",
        "for image in os.listdir(\"Pasture\"):\n",
        "    img = cv2.imread(\"Pasture/\" + image)\n",
        "    pasture.append((6, img))\n",
        "\n",
        "permanentcrop = []\n",
        "for image in os.listdir(\"PermanentCrop\"):\n",
        "    img = cv2.imread(\"PermanentCrop/\" + image)\n",
        "    permanentcrop.append((7, img))\n",
        "\n",
        "residential = []\n",
        "for image in os.listdir(\"Residential\"):\n",
        "    img = cv2.imread(\"Residential/\" + image)\n",
        "    residential.append((8, img))\n",
        "\n",
        "sealake = []\n",
        "for image in os.listdir(\"SeaLake\"):\n",
        "    img = cv2.imread(\"SeaLake/\" + image)\n",
        "    sealake.append((9, img))"
      ],
      "metadata": {
        "id": "CiuUeRMVjFis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add all lists together\n",
        "combined_data = forest + river + annualcrop + herbaceousvegetation + highway + industrial + pasture + permanentcrop + residential + sealake"
      ],
      "metadata": {
        "id": "CsCg6rPTUD4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data length\n",
        "print(len(combined_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qy5FqBstvOX",
        "outputId": "94acc22c-67d2-46e1-90cb-3c33ac4783af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels and images\n",
        "labels = [item[0] for item in combined_data]\n",
        "images = [item[1] for item in combined_data]"
      ],
      "metadata": {
        "id": "VroU5x--pC7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sava data to save time later\n",
        "#images_tensor = torch.tensor(images)\n",
        "#labels_tensor = torch.tensor(labels)\n",
        "\n",
        "#torch.save((images_tensor, labels_tensor), \"EuroSATdataset.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOrsBQOOrnR_",
        "outputId": "945a6cdc-7ff4-414b-c47f-b5187d930d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2958405580.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  images_tensor = torch.tensor(images)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data again\n",
        "images, labels = torch.load(\"EuroSATdataset.pt\")"
      ],
      "metadata": {
        "id": "E6KxW3Fcrrjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract half the dataset\n",
        "X_half1, X_half2, y_half1, y_half2 = train_test_split(images, labels, test_size=0.5, stratify=labels, random_state=42)\n",
        "\n",
        "# Split the data in train and test\n",
        "X_train, X_test1, y_train, y_test1 = train_test_split(X_half1, y_half1, test_size=0.2, stratify=y_half1, random_state=42)\n",
        "\n",
        "# Combine test sets\n",
        "X_test = np.concatenate([X_test1, X_half2])\n",
        "y_test = np.concatenate([y_test1, y_half2])"
      ],
      "metadata": {
        "id": "WtNTBCHdJ39A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "metadata": {
        "id": "oJFAl8qLeZ25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa35b956-c5c7-405e-de2e-9a748da0e554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10800\n",
            "16200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "# Resize images to 224x224\n",
        "X_train_resized = F.interpolate(X_train_t, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "X_test_resized  = F.interpolate(X_test_t, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "print(\"Resized X_train shape:\", X_train_resized.shape)\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
        "std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
        "\n",
        "X_train_resized = (X_train_resized / 255.0 - mean) / std\n",
        "X_test_resized  = (X_test_resized  / 255.0 - mean) / std\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_resized, y_train_t)\n",
        "test_dataset  = TensorDataset(X_test_resized, y_test_t)\n",
        "\n",
        "# Create dataloaders with batch size 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmbv9ZMGiZdv",
        "outputId": "d15419ca-8b0b-4a47-aecb-d595dbb42759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (10800, 64, 64, 3)\n",
            "y_train shape: (10800,)\n",
            "X_test shape: (16200, 64, 64, 3)\n",
            "y_test shape: (16200,)\n",
            "Resized X_train shape: torch.Size([10800, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the pre-trained models on half the dataset"
      ],
      "metadata": {
        "id": "xpPokjeuL1w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision transformer (ViT) (Dosovitskiy et al., 2021)\n",
        "Code source: https://huggingface.co/timm/vit_base_patch16_224.mae\n"
      ],
      "metadata": {
        "id": "i98ACMg549uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=10)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "NqXFKDLJ7UEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccf479d-d11e-4768-efe4-b8a6462725a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58145029-6afd-44f7-b66c-cf4219a92adf",
        "id": "SEwq8aX649uS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85806346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "M3yRSpSk49uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "outputId": "5c733dda-9f83-40d8-cbc2-af21bf56a728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE1ELFvN49uS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.2362\n",
            "Epoch [2/20] - Loss: 0.0455\n",
            "Epoch [3/20] - Loss: 0.0187\n",
            "Epoch [4/20] - Loss: 0.0070\n",
            "Epoch [5/20] - Loss: 0.0018\n",
            "Epoch [6/20] - Loss: 0.0010\n",
            "Epoch [7/20] - Loss: 0.0007\n",
            "Epoch [8/20] - Loss: 0.0006\n",
            "Epoch [9/20] - Loss: 0.0005\n",
            "Epoch [10/20] - Loss: 0.0004\n",
            "Epoch [11/20] - Loss: 0.0004\n",
            "Epoch [12/20] - Loss: 0.0003\n",
            "Epoch [13/20] - Loss: 0.0003\n",
            "Epoch [14/20] - Loss: 0.0003\n",
            "Epoch [15/20] - Loss: 0.0003\n",
            "Epoch [16/20] - Loss: 0.0002\n",
            "Epoch [17/20] - Loss: 0.0002\n",
            "Epoch [18/20] - Loss: 0.0002\n",
            "Epoch [19/20] - Loss: 0.0002\n",
            "Epoch [20/20] - Loss: 0.0002\n",
            "\n",
            "Training completed in: 1520.45 seconds (25.34 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "outputId": "41d845a6-c3ba-486d-95f2-9fb842e7038b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Q6Afdc49uS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compact Convolutional Transformer (CCT) (Hassani et al., 2022)\n",
        "Code source: https://github.com/SHI-Labs/Compact-Transformers/blob/main/README.md"
      ],
      "metadata": {
        "id": "voi7i3JGMIFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/SHI-Labs/Compact-Transformers.git\n",
        "%cd /content/drive/My Drive/EuroSAT_RGB/Compact-Transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncx4GJqsMQ2R",
        "outputId": "ff358ff1-52ae-4c3e-f4f0-da8cb27b8c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/EuroSAT_RGB/Compact-Transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src import cct_14_7x2_224\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = len(torch.unique(y_train_t))\n",
        "\n",
        "model = cct_14_7x2_224(pretrained=True, progress=True, num_classes=num_classes)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SphpXvOk1v_",
        "outputId": "d4b76030-5933-4b98-dada-dd2c140fccb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth\" to /root/.cache/torch/hub/checkpoints/cct_14_7x2_224_imagenet.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85.4M/85.4M [00:01<00:00, 57.3MB/s]\n",
            "WARNING:train:Removing classifier.fc.weight, number of classes has changed.\n",
            "WARNING:train:Removing classifier.fc.bias, number of classes has changed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-42JT9-GbzXx",
        "outputId": "a85e28a1-5ec7-40dd-e734-1a0cf6e7a34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21986123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "VWCG4C5IiST3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJg8KJ9slaKs",
        "outputId": "de8fa1f2-e72a-4ab0-b4cd-174c7edee383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.8866\n",
            "Epoch [2/20] - Loss: 0.2951\n",
            "Epoch [3/20] - Loss: 0.2046\n",
            "Epoch [4/20] - Loss: 0.1512\n",
            "Epoch [5/20] - Loss: 0.1230\n",
            "Epoch [6/20] - Loss: 0.1085\n",
            "Epoch [7/20] - Loss: 0.0880\n",
            "Epoch [8/20] - Loss: 0.1478\n",
            "Epoch [9/20] - Loss: 0.1039\n",
            "Epoch [10/20] - Loss: 0.0677\n",
            "Epoch [11/20] - Loss: 0.0575\n",
            "Epoch [12/20] - Loss: 0.0521\n",
            "Epoch [13/20] - Loss: 0.0450\n",
            "Epoch [14/20] - Loss: 0.0443\n",
            "Epoch [15/20] - Loss: 0.0419\n",
            "Epoch [16/20] - Loss: 0.0331\n",
            "Epoch [17/20] - Loss: 0.0323\n",
            "Epoch [18/20] - Loss: 0.0297\n",
            "Epoch [19/20] - Loss: 0.0260\n",
            "Epoch [20/20] - Loss: 0.0258\n",
            "\n",
            "Training completed in: 673.12 seconds (11.22 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JLHFKh8lpGY",
        "outputId": "3784edeb-c4f6-4a08-b6ad-e5e3f8bd1d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 97.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data-efficient image Transformers (DeiT) (Touvron et al., 2021)\n",
        "Code source: https://github.com/facebookresearch/deit/blob/main/README_deit.md"
      ],
      "metadata": {
        "id": "8p7PNU9EMREK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = len(torch.unique(y_train_t))\n",
        "\n",
        "model = timm.create_model('deit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "vqdUvIBn3zfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d2ecc86f4a574182ad122d9e2ea0710b",
            "a47115a941b149f5abd568fa7f9624fa",
            "6f359b4fb03249aa9b863891e91609a1",
            "482a1566d18941808a8d7e7f137ee532",
            "16e7c9ee05ea4a56bd7a678983d522f7",
            "f187cefdb5684ac1b15558a8b588f056",
            "98f1e4281c4640bf9ed6bbd0dca44ffa",
            "7424c418f6b4468292ecd58fc4f6858e",
            "bbcab7a5679a480f8b9e7ea1dc36a217",
            "22ab44ad023848d28dd722a43ff9b332",
            "5980e9e5e47f4a1aa0f13c15c7f9eb54"
          ]
        },
        "outputId": "89dbcf28-6cb6-4469-840c-1157fc55bc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2ecc86f4a574182ad122d9e2ea0710b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ECzko6ib1um",
        "outputId": "fd9e2a93-1e44-4c5f-9567-0e0e4f45b13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85806346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "zl48FVds4Ujr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvjgt7H4-rn",
        "outputId": "e8b5b097-daf4-4949-c4de-e7a29a8e6024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.7423\n",
            "Epoch [2/20] - Loss: 0.1692\n",
            "Epoch [3/20] - Loss: 0.1003\n",
            "Epoch [4/20] - Loss: 0.0676\n",
            "Epoch [5/20] - Loss: 0.0490\n",
            "Epoch [6/20] - Loss: 0.0364\n",
            "Epoch [7/20] - Loss: 0.0278\n",
            "Epoch [8/20] - Loss: 0.0210\n",
            "Epoch [9/20] - Loss: 0.0163\n",
            "Epoch [10/20] - Loss: 0.0124\n",
            "Epoch [11/20] - Loss: 0.0101\n",
            "Epoch [12/20] - Loss: 0.0083\n",
            "Epoch [13/20] - Loss: 0.0069\n",
            "Epoch [14/20] - Loss: 0.0059\n",
            "Epoch [15/20] - Loss: 0.0052\n",
            "Epoch [16/20] - Loss: 0.0046\n",
            "Epoch [17/20] - Loss: 0.0041\n",
            "Epoch [18/20] - Loss: 0.0037\n",
            "Epoch [19/20] - Loss: 0.0033\n",
            "Epoch [20/20] - Loss: 0.0031\n",
            "\n",
            "Training completed in: 1518.67 seconds (25.31 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x622dLf5ASF",
        "outputId": "e17c02e2-dff8-46e7-87eb-019abb5f99f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convolution-Enhanced Image Transformer (CeiT) (Yuan et al., 2021)\n",
        "Code source: https://github.com/coeusguo/ceit"
      ],
      "metadata": {
        "id": "HoPQOy07MRyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CeiT Code"
      ],
      "metadata": {
        "id": "gQumSMveip2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from timm.models.vision_transformer import default_cfgs, _cfg\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'ceit_tiny_patch16_224', 'ceit_small_patch16_224', 'ceit_base_patch16_224',\n",
        "    'ceit_tiny_patch16_384', 'ceit_small_patch16_384',\n",
        "]\n",
        "\n",
        "\n",
        "class Image2Tokens(nn.Module):\n",
        "    def __init__(self, in_chans=3, out_chans=64, kernel_size=7, stride=2):\n",
        "        super(Image2Tokens, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_chans, out_chans, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=kernel_size // 2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_chans)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LocallyEnhancedFeedForward(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,\n",
        "                 kernel_size=3, with_bn=True):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        # pointwise\n",
        "        self.conv1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, stride=1, padding=0)\n",
        "        # depthwise\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            hidden_features, hidden_features, kernel_size=kernel_size, stride=1,\n",
        "            padding=(kernel_size - 1) // 2, groups=hidden_features\n",
        "        )\n",
        "        # pointwise\n",
        "        self.conv3 = nn.Conv2d(hidden_features, out_features, kernel_size=1, stride=1, padding=0)\n",
        "        self.act = act_layer()\n",
        "        # self.drop = nn.Dropout(drop)\n",
        "\n",
        "        self.with_bn = with_bn\n",
        "        if self.with_bn:\n",
        "            self.bn1 = nn.BatchNorm2d(hidden_features)\n",
        "            self.bn2 = nn.BatchNorm2d(hidden_features)\n",
        "            self.bn3 = nn.BatchNorm2d(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, k = x.size()\n",
        "        cls_token, tokens = torch.split(x, [1, n - 1], dim=1)\n",
        "        x = tokens.reshape(b, int(math.sqrt(n - 1)), int(math.sqrt(n - 1)), k).permute(0, 3, 1, 2)\n",
        "        if self.with_bn:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.act(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.bn2(x)\n",
        "            x = self.act(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.bn3(x)\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.act(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.act(x)\n",
        "            x = self.conv3(x)\n",
        "\n",
        "        tokens = x.flatten(2).permute(0, 2, 1)\n",
        "        out = torch.cat((cls_token, tokens), dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.attention_map = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # self.attention_map = attn\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionLCA(Attention):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super(AttentionLCA, self).__init__(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
        "        self.dim = dim\n",
        "        self.qkv_bias = qkv_bias\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        q_weight = self.qkv.weight[:self.dim, :]\n",
        "        q_bias = None if not self.qkv_bias else self.qkv.bias[:self.dim]\n",
        "        kv_weight = self.qkv.weight[self.dim:, :]\n",
        "        kv_bias = None if not self.qkv_bias else self.qkv.bias[self.dim:]\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        _, last_token = torch.split(x, [N-1, 1], dim=1)\n",
        "\n",
        "        q = F.linear(last_token, q_weight, q_bias)\\\n",
        "             .reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        kv = F.linear(x, kv_weight, kv_bias)\\\n",
        "              .reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # self.attention_map = attn\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, kernel_size=3, with_bn=True,\n",
        "                 feedforward_type='leff'):\n",
        "        super().__init__()\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.feedforward_type = feedforward_type\n",
        "\n",
        "        if feedforward_type == 'leff':\n",
        "            self.attn = Attention(\n",
        "                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "            self.leff = LocallyEnhancedFeedForward(\n",
        "                in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,\n",
        "                kernel_size=kernel_size, with_bn=with_bn,\n",
        "            )\n",
        "        else:  # LCA\n",
        "            self.attn = AttentionLCA(\n",
        "                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "            self.feedforward = Mlp(\n",
        "                in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.feedforward_type == 'leff':\n",
        "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.leff(self.norm2(x)))\n",
        "            return x, x[:, 0]\n",
        "        else:  # LCA\n",
        "            _, last_token = torch.split(x, [x.size(1)-1, 1], dim=1)\n",
        "            x = last_token + self.drop_path(self.attn(self.norm1(x)))\n",
        "            x = x + self.drop_path(self.feedforward(self.norm2(x)))\n",
        "            return x\n",
        "\n",
        "\n",
        "class HybridEmbed(nn.Module):\n",
        "    \"\"\" CNN Feature Map Embedding\n",
        "    Extract feature map from CNN, flatten, project to embedding dim.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, img_size=224, patch_size=16, feature_size=None, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        assert isinstance(backbone, nn.Module)\n",
        "        img_size = to_2tuple(img_size)\n",
        "        self.img_size = img_size\n",
        "        self.backbone = backbone\n",
        "        if feature_size is None:\n",
        "            with torch.no_grad():\n",
        "                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
        "                # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
        "                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
        "                training = backbone.training\n",
        "                if training:\n",
        "                    backbone.eval()\n",
        "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n",
        "                if isinstance(o, (list, tuple)):\n",
        "                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n",
        "                feature_size = o.shape[-2:]\n",
        "                feature_dim = o.shape[1]\n",
        "                backbone.train(training)\n",
        "        else:\n",
        "            feature_size = to_2tuple(feature_size)\n",
        "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
        "        print('feature_size is {}, feature_dim is {}, patch_size is {}'.format(\n",
        "            feature_size, feature_dim, patch_size\n",
        "        ))\n",
        "        self.num_patches = (feature_size[0] // patch_size) * (feature_size[1] // patch_size)\n",
        "        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CeIT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_chans=3,\n",
        "                 num_classes=1000,\n",
        "                 embed_dim=768,\n",
        "                 depth=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=None,\n",
        "                 drop_rate=0.,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 hybrid_backbone=None,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 leff_local_size=3,\n",
        "                 leff_with_bn=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            - img_size (:obj:`int`): input image size\n",
        "            - patch_size (:obj:`int`): patch size\n",
        "            - in_chans (:obj:`int`): input channels\n",
        "            - num_classes (:obj:`int`): number of classes\n",
        "            - embed_dim (:obj:`int`): embedding dimensions for tokens\n",
        "            - depth (:obj:`int`): depth of encoder\n",
        "            - num_heads (:obj:`int`): number of heads in multi-head self-attention\n",
        "            - mlp_ratio (:obj:`float`): expand ratio in feedforward\n",
        "            - qkv_bias (:obj:`bool`): whether to add bias for mlp of qkv\n",
        "            - qk_scale (:obj:`float`): scale ratio for qk, default is head_dim ** -0.5\n",
        "            - drop_rate (:obj:`float`): dropout rate in feedforward module after linear operation\n",
        "                and projection drop rate in attention\n",
        "            - attn_drop_rate (:obj:`float`): dropout rate for attention\n",
        "            - drop_path_rate (:obj:`float`): drop_path rate after attention\n",
        "            - hybrid_backbone (:obj:`nn.Module`): backbone e.g. resnet\n",
        "            - norm_layer (:obj:`nn.Module`): normalization type\n",
        "            - leff_local_size (:obj:`int`): kernel size in LocallyEnhancedFeedForward\n",
        "            - leff_with_bn (:obj:`bool`): whether add bn in LocallyEnhancedFeedForward\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "\n",
        "        self.i2t = HybridEmbed(\n",
        "            hybrid_backbone, img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.i2t.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
        "                kernel_size=leff_local_size, with_bn=leff_with_bn)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        # without droppath\n",
        "        self.lca = Block(\n",
        "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0., norm_layer=norm_layer,\n",
        "            feedforward_type = 'lca'\n",
        "        )\n",
        "        self.pos_layer_embed = nn.Parameter(torch.zeros(1, depth, embed_dim))\n",
        "\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
        "        # self.repr = nn.Linear(embed_dim, representation_size)\n",
        "        # self.repr_act = nn.Tanh()\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.i2t(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        cls_token_list = []\n",
        "        for blk in self.blocks:\n",
        "            x, curr_cls_token = blk(x)\n",
        "            cls_token_list.append(curr_cls_token)\n",
        "\n",
        "        all_cls_token = torch.stack(cls_token_list, dim=1)  # B*D*K\n",
        "        all_cls_token = all_cls_token + self.pos_layer_embed\n",
        "        # attention over cls tokens\n",
        "        last_cls_token = self.lca(all_cls_token)\n",
        "        last_cls_token = self.norm(last_cls_token)\n",
        "\n",
        "        return last_cls_token.view(B, -1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "def ceit_tiny_patch16_224(pretrained=False, **kwargs):\n",
        "    \"\"\"\n",
        "    convolutional + pooling stem\n",
        "    local enhanced feedforward\n",
        "    attention over cls_tokens\n",
        "    \"\"\"\n",
        "    i2t = Image2Tokens()\n",
        "    model = CeIT(\n",
        "        hybrid_backbone=i2t,\n",
        "        patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "    return model\n",
        "\n",
        "def ceit_base_patch16_224(pretrained=False, **kwargs):\n",
        "    \"\"\"\n",
        "    convolutional + pooling stem\n",
        "    local enhanced feedforward\n",
        "    attention over cls_tokens\n",
        "    \"\"\"\n",
        "    i2t = Image2Tokens()\n",
        "    model = CeIT(\n",
        "        hybrid_backbone=i2t,\n",
        "        patch_size=4, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = _cfg()\n",
        "    return model"
      ],
      "metadata": {
        "id": "aImND1FUMSAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0830935-f3f0-41ad-b74f-630987894d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ceit initialization and training"
      ],
      "metadata": {
        "id": "rmJuNRnhixmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/EuroSAT_RGB/\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ceit_base_patch16_224(num_classes=10).to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/EuroSAT_RGB/ceit_base_checkpoint.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Load the state dict\n",
        "state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "\n",
        "# Delete heads to make the state dict fit\n",
        "for k in list(state_dict.keys()):\n",
        "    if 'head.' in k:\n",
        "        del state_dict[k]\n",
        "\n",
        "# Apply state dict to the model\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSJArleMwLSS",
        "outputId": "6395eff1-2729-407d-8c2b-7c784ccd7f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/EuroSAT_RGB\n",
            "feature_size is torch.Size([56, 56]), feature_dim is 64, patch_size is 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "K9aGmrmStnFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c44f3c-5bb5-48b5-a8d6-25a9fa6d7123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93644106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "nMSGKD4hsyW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtXvhrYFsyaC",
        "outputId": "92adbcd5-fe24-499e-d3bf-ba5f794e801b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 1.0103\n",
            "Epoch [2/20] - Loss: 0.4034\n",
            "Epoch [3/20] - Loss: 0.2674\n",
            "Epoch [4/20] - Loss: 0.2009\n",
            "Epoch [5/20] - Loss: 0.1643\n",
            "Epoch [6/20] - Loss: 0.1400\n",
            "Epoch [7/20] - Loss: 0.1222\n",
            "Epoch [8/20] - Loss: 0.1070\n",
            "Epoch [9/20] - Loss: 0.0982\n",
            "Epoch [10/20] - Loss: 0.0860\n",
            "Epoch [11/20] - Loss: 0.0767\n",
            "Epoch [12/20] - Loss: 0.0701\n",
            "Epoch [13/20] - Loss: 0.0629\n",
            "Epoch [14/20] - Loss: 0.0606\n",
            "Epoch [15/20] - Loss: 0.0541\n",
            "Epoch [16/20] - Loss: 0.0524\n",
            "Epoch [17/20] - Loss: 0.0478\n",
            "Epoch [18/20] - Loss: 0.0485\n",
            "Epoch [19/20] - Loss: 0.0437\n",
            "Epoch [20/20] - Loss: 0.0404\n",
            "\n",
            "Training completed in: 1180.33 seconds (19.67 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "csDDbQjTsyc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa92462e-43cc-4359-955f-56f3d664a5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 97.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LocalViT (Yawei Li et al., 2021)\n",
        "Code source: https://github.com/ofsoundof/LocalViT"
      ],
      "metadata": {
        "id": "v0Q_l97BMSIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LocalViT Code"
      ],
      "metadata": {
        "id": "m7mAFjMApujD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Yawei Li\n",
        "Email: yawei.li@vision.ee.ethz.ch\n",
        "\n",
        "Introducing locality mechanism to \"DeiT: Data-efficient Image Transformers\".\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from functools import partial\n",
        "from timm.models.vision_transformer import VisionTransformer\n",
        "from timm.models.layers import DropPath\n",
        "from timm.models.registry import register_model\n",
        "import collections.abc as container_abcs\n",
        "\n",
        "\n",
        "class h_sigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_sigmoid, self).__init__()\n",
        "        self.relu = nn.ReLU6(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + 3) / 6\n",
        "\n",
        "\n",
        "class h_swish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(h_swish, self).__init__()\n",
        "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.sigmoid(x)\n",
        "\n",
        "\n",
        "class ECALayer(nn.Module):\n",
        "    def __init__(self, channel, gamma=2, b=1, sigmoid=True):\n",
        "        super(ECALayer, self).__init__()\n",
        "        t = int(abs((math.log(channel, 2) + b) / gamma))\n",
        "        k = t if t % 2 else t + 1\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=k // 2, bias=False)\n",
        "        if sigmoid:\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "        else:\n",
        "            self.sigmoid = h_sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv(y.squeeze(-1).transpose(-1, -2))\n",
        "        y = y.transpose(-1, -2).unsqueeze(-1)\n",
        "        y = self.sigmoid(y)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(channel, channel // reduction),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(channel // reduction, channel),\n",
        "                h_sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class LocalityFeedForward(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, stride, expand_ratio=4., act='hs+se', reduction=4,\n",
        "                 wo_dp_conv=False, dp_first=False):\n",
        "        \"\"\"\n",
        "        :param in_dim: the input dimension\n",
        "        :param out_dim: the output dimension. The input and output dimension should be the same.\n",
        "        :param stride: stride of the depth-wise convolution.\n",
        "        :param expand_ratio: expansion ratio of the hidden dimension.\n",
        "        :param act: the activation function.\n",
        "                    relu: ReLU\n",
        "                    hs: h_swish\n",
        "                    hs+se: h_swish and SE module\n",
        "                    hs+eca: h_swish and ECA module\n",
        "                    hs+ecah: h_swish and ECA module. Compared with eca, h_sigmoid is used.\n",
        "        :param reduction: reduction rate in SE module.\n",
        "        :param wo_dp_conv: without depth-wise convolution.\n",
        "        :param dp_first: place depth-wise convolution as the first layer.\n",
        "        \"\"\"\n",
        "        super(LocalityFeedForward, self).__init__()\n",
        "        hidden_dim = int(in_dim * expand_ratio)\n",
        "        kernel_size = 3\n",
        "\n",
        "        layers = []\n",
        "        # the first linear layer is replaced by 1x1 convolution.\n",
        "        layers.extend([\n",
        "            nn.Conv2d(in_dim, hidden_dim, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)])\n",
        "\n",
        "        # the depth-wise convolution between the two linear layers\n",
        "        if not wo_dp_conv:\n",
        "            dp = [\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, kernel_size // 2, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                h_swish() if act.find('hs') >= 0 else nn.ReLU6(inplace=True)\n",
        "            ]\n",
        "            if dp_first:\n",
        "                layers = dp + layers\n",
        "            else:\n",
        "                layers.extend(dp)\n",
        "\n",
        "        if act.find('+') >= 0:\n",
        "            attn = act.split('+')[1]\n",
        "            if attn == 'se':\n",
        "                layers.append(SELayer(hidden_dim, reduction=reduction))\n",
        "            elif attn.find('eca') >= 0:\n",
        "                layers.append(ECALayer(hidden_dim, sigmoid=attn == 'eca'))\n",
        "            else:\n",
        "                raise NotImplementedError('Activation type {} is not implemented'.format(act))\n",
        "\n",
        "        # the second linear layer is replaced by 1x1 convolution.\n",
        "        layers.extend([\n",
        "            nn.Conv2d(hidden_dim, out_dim, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(out_dim)\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, qk_reduce=1, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        :param dim:\n",
        "        :param num_heads:\n",
        "        :param qkv_bias:\n",
        "        :param qk_scale:\n",
        "        :param qk_reduce: reduce the output dimension for QK projection\n",
        "        :param attn_drop:\n",
        "        :param proj_drop:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qk_reduce = qk_reduce\n",
        "        self.dim = dim\n",
        "        self.qk_dim = int(dim / self.qk_reduce)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, int(dim * (1 + 1 / qk_reduce * 2)), bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        if self.qk_reduce == 1:\n",
        "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        else:\n",
        "            q, k, v = torch.split(self.qkv(x), [self.qk_dim, self.qk_dim, self.dim], dim=-1)\n",
        "            q = q.reshape(B, N, self.num_heads, -1).transpose(1, 2)\n",
        "            k = k.reshape(B, N, self.num_heads, -1).transpose(1, 2)\n",
        "            v = v.reshape(B, N, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, qk_reduce=1, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, act='hs+se', reduction=4, wo_dp_conv=False, dp_first=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, qk_reduce=qk_reduce,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        # The MLP is replaced by the conv layers.\n",
        "        self.conv = LocalityFeedForward(dim, dim, 1, mlp_ratio, act, reduction, wo_dp_conv, dp_first)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_token, embed_dim = x.shape                                  # (B, 197, dim)\n",
        "        patch_size = int(math.sqrt(num_token))\n",
        "\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))                            # (B, 197, dim)\n",
        "        # Split the class token and the image token.\n",
        "        cls_token, x = torch.split(x, [1, num_token - 1], dim=1)                    # (B, 1, dim), (B, 196, dim)\n",
        "        # Reshape and update the image token.\n",
        "        x = x.transpose(1, 2).view(batch_size, embed_dim, patch_size, patch_size)   # (B, dim, 14, 14)\n",
        "        x = self.conv(x).flatten(2).transpose(1, 2)                                 # (B, 196, dim)\n",
        "        # Concatenate the class token and the newly computed image token.\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        #########################################\n",
        "        # Origianl implementation\n",
        "        # self.norm2 = norm_layer(dim)\n",
        "        #         mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        #         self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        #########################################\n",
        "\n",
        "        # Replace the MLP layer by LocalityFeedForward.\n",
        "        self.conv = LocalityFeedForward(dim, dim, 1, mlp_ratio, act='hs+se', reduction=dim//4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        #########################################\n",
        "        # Origianl implementation\n",
        "        # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        #########################################\n",
        "\n",
        "        # Change the computation accordingly in three steps.\n",
        "        batch_size, num_token, embed_dim = x.shape\n",
        "        patch_size = int(math.sqrt(num_token))\n",
        "        # 1. Split the class token and the image token.\n",
        "        cls_token, x = torch.split(x, [1, embed_dim - 1], dim=1)\n",
        "        # 2. Reshape and update the image token.\n",
        "        x = x.transpose(1, 2).view(batch_size, embed_dim, patch_size, patch_size)\n",
        "        x = self.conv(x).flatten(2).transpose(1, 2)\n",
        "        # 3. Concatenate the class token and the newly computed image token.\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LocalVisionTransformer(VisionTransformer):\n",
        "    \"\"\" Vision Transformer with Locality-enhanced FeedForward blocks \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "        qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "        drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm,\n",
        "        act=3, reduction=4, wo_dp_conv=False, dp_first=False\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            num_classes=num_classes,\n",
        "            embed_dim=embed_dim,\n",
        "            depth=depth,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            qkv_bias=qkv_bias,\n",
        "            drop_rate=drop_rate,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "\n",
        "        # Parse activation mode\n",
        "        if act == 1: act = 'relu6'\n",
        "        elif act == 2: act = 'hs'\n",
        "        elif act == 3: act = 'hs+se'\n",
        "        elif act == 4: act = 'hs+eca'\n",
        "        else: act = 'hs+ecah'\n",
        "\n",
        "        # Replace standard transformer blocks with locality-enhanced blocks\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n",
        "                norm_layer=norm_layer, act=act, reduction=reduction,\n",
        "                wo_dp_conv=wo_dp_conv, dp_first=dp_first\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Class token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Positional embedding + dropout\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Local ViT blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Final norm + classifier head\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "@register_model\n",
        "def localvit_tiny_mlp4_act3_r192(pretrained=False, **kwargs):\n",
        "    model = LocalVisionTransformer(\n",
        "        patch_size=16, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, qkv_bias=True, act=3, reduction=192,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "@register_model\n",
        "def localvit_small_mlp4_act3_r384(pretrained=False, **kwargs):\n",
        "    model = LocalVisionTransformer(\n",
        "        patch_size=16, embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, qkv_bias=True, act=3, reduction=384,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tg2pTFWKWaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LocalViT initialization and training"
      ],
      "metadata": {
        "id": "KMkCDia2p4a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = localvit_small_mlp4_act3_r384()\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "GCqzVCQhuqz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/EuroSAT_RGB/\n",
        "\n",
        "# Load the state dict\n",
        "checkpoint = torch.load(\"localvit_s.pth\", map_location='cpu', weights_only=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIAh5TguuqRB",
        "outputId": "2a80e784-bc0f-4315-982d-668512d9dc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/EuroSAT_RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract model weights from checkpoint\n",
        "if 'model' in checkpoint:\n",
        "    state_dict = checkpoint['model']\n",
        "else:\n",
        "    state_dict = checkpoint"
      ],
      "metadata": {
        "id": "6iAd1c8LuqxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply weights to the model\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAWZ3_bIuq2w",
        "outputId": "678a05a5-15c2-42cf-84b4-ca18f799287d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLFkMXHzuAPj",
        "outputId": "607216a1-d05e-4d14-e4bb-51bcfcd8a48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22433176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "khlz2VJoVHiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brg_813gC1ZV",
        "outputId": "489b6ef9-e91f-4989-eb79-16526aae5de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 0.8674\n",
            "Epoch [2/20] - Loss: 0.2224\n",
            "Epoch [3/20] - Loss: 0.1525\n",
            "Epoch [4/20] - Loss: 0.1032\n",
            "Epoch [5/20] - Loss: 0.0780\n",
            "Epoch [6/20] - Loss: 0.0664\n",
            "Epoch [7/20] - Loss: 0.0575\n",
            "Epoch [8/20] - Loss: 0.0473\n",
            "Epoch [9/20] - Loss: 0.0384\n",
            "Epoch [10/20] - Loss: 0.0275\n",
            "Epoch [11/20] - Loss: 0.0258\n",
            "Epoch [12/20] - Loss: 0.0271\n",
            "Epoch [13/20] - Loss: 0.0216\n",
            "Epoch [14/20] - Loss: 0.0205\n",
            "Epoch [15/20] - Loss: 0.0169\n",
            "Epoch [16/20] - Loss: 0.0176\n",
            "Epoch [17/20] - Loss: 0.0112\n",
            "Epoch [18/20] - Loss: 0.0114\n",
            "Epoch [19/20] - Loss: 0.0106\n",
            "Epoch [20/20] - Loss: 0.0114\n",
            "\n",
            "Training completed in: 794.19 seconds (13.24 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "pD2xoA3yVHtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b209bae3-b0ba-4d5f-dc37-44cb5ac1498b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 97.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conformer (Peng et al., 2021)\n",
        "Code source: https://github.com/pengzhiliang/Conformer"
      ],
      "metadata": {
        "id": "cgMDD6F4ME9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conformer Code"
      ],
      "metadata": {
        "id": "xj2DCnMFqIn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n",
        "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        expansion = 4\n",
        "        med_planes = outplanes // expansion\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = norm_layer(med_planes)\n",
        "        self.act1 = act_layer(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(med_planes)\n",
        "        self.act2 = act_layer(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = norm_layer(outplanes)\n",
        "        self.act3 = act_layer(inplace=True)\n",
        "\n",
        "        if res_conv:\n",
        "            self.residual_conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "            self.residual_bn = norm_layer(outplanes)\n",
        "\n",
        "        self.res_conv = res_conv\n",
        "        self.drop_block = drop_block\n",
        "        self.drop_path = drop_path\n",
        "\n",
        "    def zero_init_last_bn(self):\n",
        "        nn.init.zeros_(self.bn3.weight)\n",
        "\n",
        "    def forward(self, x, x_t=None, return_x_2=True):\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n",
        "        x = self.bn2(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "        x2 = self.act2(x)\n",
        "\n",
        "        x = self.conv3(x2)\n",
        "        x = self.bn3(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "\n",
        "        if self.drop_path is not None:\n",
        "            x = self.drop_path(x)\n",
        "\n",
        "        if self.res_conv:\n",
        "            residual = self.residual_conv(residual)\n",
        "            residual = self.residual_bn(residual)\n",
        "\n",
        "        x += residual\n",
        "        x = self.act3(x)\n",
        "\n",
        "        if return_x_2:\n",
        "            return x, x2\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "class FCUDown(nn.Module):\n",
        "    \"\"\" CNN feature maps -> Transformer patch embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
        "        super(FCUDown, self).__init__()\n",
        "        self.dw_stride = dw_stride\n",
        "\n",
        "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
        "        self.sample_pooling = nn.AvgPool2d(kernel_size=dw_stride, stride=dw_stride)\n",
        "\n",
        "        self.ln = norm_layer(outplanes)\n",
        "        self.act = act_layer()\n",
        "\n",
        "    def forward(self, x, x_t):\n",
        "        x = self.conv_project(x)  # [N, C, H, W]\n",
        "\n",
        "        x = self.sample_pooling(x).flatten(2).transpose(1, 2)\n",
        "        x = self.ln(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FCUUp(nn.Module):\n",
        "    \"\"\" Transformer patch embeddings -> CNN feature maps\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n",
        "                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6),):\n",
        "        super(FCUUp, self).__init__()\n",
        "\n",
        "        self.up_stride = up_stride\n",
        "        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn = norm_layer(outplanes)\n",
        "        self.act = act_layer()\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, _, C = x.shape\n",
        "        # [N, 197, 384] -> [N, 196, 384] -> [N, 384, 196] -> [N, 384, 14, 14]\n",
        "        x_r = x[:, 1:].transpose(1, 2).reshape(B, C, H, W)\n",
        "        x_r = self.act(self.bn(self.conv_project(x_r)))\n",
        "\n",
        "        return F.interpolate(x_r, size=(H * self.up_stride, W * self.up_stride))\n",
        "\n",
        "\n",
        "class Med_ConvBlock(nn.Module):\n",
        "    \"\"\" special case for Convblock with down sampling,\n",
        "    \"\"\"\n",
        "    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm2d, eps=1e-6),\n",
        "                 drop_block=None, drop_path=None):\n",
        "\n",
        "        super(Med_ConvBlock, self).__init__()\n",
        "\n",
        "        expansion = 4\n",
        "        med_planes = inplanes // expansion\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = norm_layer(med_planes)\n",
        "        self.act1 = act_layer(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(med_planes)\n",
        "        self.act2 = act_layer(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = norm_layer(inplanes)\n",
        "        self.act3 = act_layer(inplace=True)\n",
        "\n",
        "        self.drop_block = drop_block\n",
        "        self.drop_path = drop_path\n",
        "\n",
        "    def zero_init_last_bn(self):\n",
        "        nn.init.zeros_(self.bn3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        if self.drop_block is not None:\n",
        "            x = self.drop_block(x)\n",
        "\n",
        "        if self.drop_path is not None:\n",
        "            x = self.drop_path(x)\n",
        "\n",
        "        x += residual\n",
        "        x = self.act3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTransBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic module for ConvTransformer, keep feature maps for CNN block and patch embeddings for transformer encoder block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "                 last_fusion=False, num_med_block=0, groups=1):\n",
        "\n",
        "        super(ConvTransBlock, self).__init__()\n",
        "        expansion = 4\n",
        "        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, res_conv=res_conv, stride=stride, groups=groups)\n",
        "\n",
        "        if last_fusion:\n",
        "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=2, res_conv=True, groups=groups)\n",
        "        else:\n",
        "            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups)\n",
        "\n",
        "        if num_med_block > 0:\n",
        "            self.med_block = []\n",
        "            for i in range(num_med_block):\n",
        "                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups))\n",
        "            self.med_block = nn.ModuleList(self.med_block)\n",
        "\n",
        "        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n",
        "\n",
        "        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride)\n",
        "\n",
        "        self.trans_block = Block(\n",
        "            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate)\n",
        "\n",
        "        self.dw_stride = dw_stride\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_med_block = num_med_block\n",
        "        self.last_fusion = last_fusion\n",
        "\n",
        "    def forward(self, x, x_t):\n",
        "        x, x2 = self.cnn_block(x)\n",
        "\n",
        "        _, _, H, W = x2.shape\n",
        "\n",
        "        x_st = self.squeeze_block(x2, x_t)\n",
        "\n",
        "        x_t = self.trans_block(x_st + x_t)\n",
        "\n",
        "        if self.num_med_block > 0:\n",
        "            for m in self.med_block:\n",
        "                x = m(x)\n",
        "\n",
        "        x_t_r = self.expand_block(x_t, H // self.dw_stride, W // self.dw_stride)\n",
        "        x = self.fusion_block(x, x_t_r, return_x_2=False)\n",
        "\n",
        "        return x, x_t\n",
        "\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "\n",
        "    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, base_channel=64, channel_ratio=4, num_med_block=0,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n",
        "\n",
        "        # Transformer\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        assert depth % 3 == 0\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "\n",
        "        # Classifier head\n",
        "        self.trans_norm = nn.LayerNorm(embed_dim)\n",
        "        self.trans_cls_head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_cls_head = nn.Linear(int(256 * channel_ratio), num_classes)\n",
        "\n",
        "        # Stem stage: get the feature maps by conv block (copied form resnet.py)\n",
        "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 / 2 [112, 112]\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 1 / 4 [56, 56]\n",
        "\n",
        "        # 1 stage\n",
        "        stage_1_channel = int(base_channel * channel_ratio)\n",
        "        trans_dw_stride = patch_size // 4\n",
        "        self.conv_1 = ConvBlock(inplanes=64, outplanes=stage_1_channel, res_conv=True, stride=1)\n",
        "        self.trans_patch_conv = nn.Conv2d(64, embed_dim, kernel_size=trans_dw_stride, stride=trans_dw_stride, padding=0)\n",
        "        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "                             qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0],\n",
        "                             )\n",
        "\n",
        "        # 2~4 stage\n",
        "        init_stage = 2\n",
        "        fin_stage = depth // 3 + 1\n",
        "        for i in range(init_stage, fin_stage):\n",
        "            self.add_module('conv_trans_' + str(i),\n",
        "                    ConvTransBlock(\n",
        "                        stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride, embed_dim=embed_dim,\n",
        "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
        "                        num_med_block=num_med_block\n",
        "                    )\n",
        "            )\n",
        "\n",
        "\n",
        "        stage_2_channel = int(base_channel * channel_ratio * 2)\n",
        "        # 5~8 stage\n",
        "        init_stage = fin_stage # 5\n",
        "        fin_stage = fin_stage + depth // 3 # 9\n",
        "        for i in range(init_stage, fin_stage):\n",
        "            s = 2 if i == init_stage else 1\n",
        "            in_channel = stage_1_channel if i == init_stage else stage_2_channel\n",
        "            res_conv = True if i == init_stage else False\n",
        "            self.add_module('conv_trans_' + str(i),\n",
        "                    ConvTransBlock(\n",
        "                        in_channel, stage_2_channel, res_conv, s, dw_stride=trans_dw_stride // 2, embed_dim=embed_dim,\n",
        "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
        "                        num_med_block=num_med_block\n",
        "                    )\n",
        "            )\n",
        "\n",
        "        stage_3_channel = int(base_channel * channel_ratio * 2 * 2)\n",
        "        # 9~12 stage\n",
        "        init_stage = fin_stage  # 9\n",
        "        fin_stage = fin_stage + depth // 3  # 13\n",
        "        for i in range(init_stage, fin_stage):\n",
        "            s = 2 if i == init_stage else 1\n",
        "            in_channel = stage_2_channel if i == init_stage else stage_3_channel\n",
        "            res_conv = True if i == init_stage else False\n",
        "            last_fusion = True if i == depth else False\n",
        "            self.add_module('conv_trans_' + str(i),\n",
        "                    ConvTransBlock(\n",
        "                        in_channel, stage_3_channel, res_conv, s, dw_stride=trans_dw_stride // 4, embed_dim=embed_dim,\n",
        "                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n",
        "                        num_med_block=num_med_block, last_fusion=last_fusion\n",
        "                    )\n",
        "            )\n",
        "        self.fin_stage = fin_stage\n",
        "\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1.)\n",
        "            nn.init.constant_(m.bias, 0.)\n",
        "        elif isinstance(m, nn.GroupNorm):\n",
        "            nn.init.constant_(m.weight, 1.)\n",
        "            nn.init.constant_(m.bias, 0.)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token'}\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        # stem stage [N, 3, 224, 224] -> [N, 64, 56, 56]\n",
        "        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))\n",
        "\n",
        "        # 1 stage\n",
        "        x = self.conv_1(x_base, return_x_2=False)\n",
        "\n",
        "        x_t = self.trans_patch_conv(x_base).flatten(2).transpose(1, 2)\n",
        "        x_t = torch.cat([cls_tokens, x_t], dim=1)\n",
        "        x_t = self.trans_1(x_t)\n",
        "\n",
        "        # 2 ~ final\n",
        "        for i in range(2, self.fin_stage):\n",
        "            x, x_t = eval('self.conv_trans_' + str(i))(x, x_t)\n",
        "\n",
        "        # conv classification\n",
        "        x_p = self.pooling(x).flatten(1)\n",
        "        conv_cls = self.conv_cls_head(x_p)\n",
        "\n",
        "        # trans classification\n",
        "        x_t = self.trans_norm(x_t)\n",
        "        tran_cls = self.trans_cls_head(x_t[:, 0])\n",
        "\n",
        "        return [conv_cls, tran_cls]\n",
        "\n",
        "def Conformer_tiny_patch16(pretrained=False, **kwargs):\n",
        "    model = Conformer(patch_size=16, num_classes=10, channel_ratio=1, embed_dim=384, depth=12,\n",
        "                      num_heads=6, mlp_ratio=4, qkv_bias=True, **kwargs)\n",
        "    if pretrained:\n",
        "        raise NotImplementedError\n",
        "    return model\n",
        "\n",
        "\n",
        "def Conformer_base_patch16(pretrained=False, **kwargs):\n",
        "    model = Conformer(patch_size=16, channel_ratio=6, embed_dim=576, depth=12,\n",
        "                      num_heads=9, mlp_ratio=4, qkv_bias=True, **kwargs)\n",
        "    if pretrained:\n",
        "        raise NotImplementedError\n",
        "    return model"
      ],
      "metadata": {
        "id": "HDZpgIacPZKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conformer initialization and training"
      ],
      "metadata": {
        "id": "xa0rt6qUqQZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/EuroSAT_RGB/\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Conformer_base_patch16().to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/EuroSAT_RGB/Conformer_base_patch16.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Load the state dict\n",
        "state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "\n",
        "# Delete heads to make the state dict fit\n",
        "for k in list(state_dict.keys()):\n",
        "    if 'head.' in k:\n",
        "        del state_dict[k]\n",
        "\n",
        "# Apply state dict to the model\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "id": "qVjROUKS8CUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b55f8d-69fc-4a9f-e11e-de04b82f8530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/EuroSAT_RGB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['trans_cls_head.weight', 'trans_cls_head.bias', 'conv_cls_head.weight', 'conv_cls_head.bias'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "WSr2Ef8jdsi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a17d7d-5f7b-4ec3-9265-9a57281d3b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83289136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "XJieZukDcN3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)[1]\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "id": "5Oa8i9PodWQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e629d521-f572-4660-8d56-90f0809f1a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 1.2817\n",
            "Epoch [2/20] - Loss: 0.3537\n",
            "Epoch [3/20] - Loss: 0.2268\n",
            "Epoch [4/20] - Loss: 0.1665\n",
            "Epoch [5/20] - Loss: 0.1327\n",
            "Epoch [6/20] - Loss: 0.1115\n",
            "Epoch [7/20] - Loss: 0.2009\n",
            "Epoch [8/20] - Loss: 0.1579\n",
            "Epoch [9/20] - Loss: 0.1137\n",
            "Epoch [10/20] - Loss: 0.0915\n",
            "Epoch [11/20] - Loss: 0.0748\n",
            "Epoch [12/20] - Loss: 0.0724\n",
            "Epoch [13/20] - Loss: 0.0602\n",
            "Epoch [14/20] - Loss: 0.0621\n",
            "Epoch [15/20] - Loss: 0.0531\n",
            "Epoch [16/20] - Loss: 0.0507\n",
            "Epoch [17/20] - Loss: 0.0388\n",
            "Epoch [18/20] - Loss: 0.0378\n",
            "Epoch [19/20] - Loss: 0.0396\n",
            "Epoch [20/20] - Loss: 0.0319\n",
            "\n",
            "Training completed in: 1894.12 seconds (31.57 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)[1]\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "KeryVzS3cN_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065fec52-8186-4b99-d161-13309f896085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 97.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CvT (Wu et al., 2021)\n",
        "Code source: https://github.com/leoxiaobin/CvT"
      ],
      "metadata": {
        "id": "61HMqWIwIdd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CvT Code"
      ],
      "metadata": {
        "id": "kUcTBwfbvhco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from itertools import repeat\n",
        "import collections.abc as container_abcs\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "_model_entrypoints = {}\n",
        "\n",
        "\n",
        "def register_model(fn):\n",
        "    module_name_split = fn.__module__.split('.')\n",
        "    model_name = module_name_split[-1]\n",
        "\n",
        "    _model_entrypoints[model_name] = fn\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "def model_entrypoints(model_name):\n",
        "    return _model_entrypoints[model_name]\n",
        "\n",
        "\n",
        "def is_model(model_name):\n",
        "    return model_name in _model_entrypoints\n",
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, container_abcs.Iterable):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "\n",
        "    return parse\n",
        "\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "to_ntuple = _ntuple\n",
        "\n",
        "\n",
        "class LayerNorm(nn.LayerNorm):\n",
        "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        orig_type = x.dtype\n",
        "        ret = super().forward(x.type(torch.float32))\n",
        "        return ret.type(orig_type)\n",
        "\n",
        "\n",
        "class QuickGELU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x * torch.sigmoid(1.702 * x)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features=None,\n",
        "                 out_features=None,\n",
        "                 act_layer=nn.GELU,\n",
        "                 drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim_in,\n",
        "                 dim_out,\n",
        "                 num_heads,\n",
        "                 qkv_bias=False,\n",
        "                 attn_drop=0.,\n",
        "                 proj_drop=0.,\n",
        "                 method='dw_bn',\n",
        "                 kernel_size=3,\n",
        "                 stride_kv=1,\n",
        "                 stride_q=1,\n",
        "                 padding_kv=1,\n",
        "                 padding_q=1,\n",
        "                 with_cls_token=True,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.stride_kv = stride_kv\n",
        "        self.stride_q = stride_q\n",
        "        self.dim = dim_out\n",
        "        self.num_heads = num_heads\n",
        "        # head_dim = self.qkv_dim // num_heads\n",
        "        self.scale = dim_out ** -0.5\n",
        "        self.with_cls_token = with_cls_token\n",
        "\n",
        "        self.conv_proj_q = self._build_projection(\n",
        "            dim_in, dim_out, kernel_size, padding_q,\n",
        "            stride_q, 'linear' if method == 'avg' else method\n",
        "        )\n",
        "        self.conv_proj_k = self._build_projection(\n",
        "            dim_in, dim_out, kernel_size, padding_kv,\n",
        "            stride_kv, method\n",
        "        )\n",
        "        self.conv_proj_v = self._build_projection(\n",
        "            dim_in, dim_out, kernel_size, padding_kv,\n",
        "            stride_kv, method\n",
        "        )\n",
        "\n",
        "        self.proj_q = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "        self.proj_k = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "        self.proj_v = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim_out, dim_out)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def _build_projection(self,\n",
        "                          dim_in,\n",
        "                          dim_out,\n",
        "                          kernel_size,\n",
        "                          padding,\n",
        "                          stride,\n",
        "                          method):\n",
        "        if method == 'dw_bn':\n",
        "            proj = nn.Sequential(OrderedDict([\n",
        "                ('conv', nn.Conv2d(\n",
        "                    dim_in,\n",
        "                    dim_in,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding=padding,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                    groups=dim_in\n",
        "                )),\n",
        "                ('bn', nn.BatchNorm2d(dim_in)),\n",
        "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
        "            ]))\n",
        "        elif method == 'avg':\n",
        "            proj = nn.Sequential(OrderedDict([\n",
        "                ('avg', nn.AvgPool2d(\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding=padding,\n",
        "                    stride=stride,\n",
        "                    ceil_mode=True\n",
        "                )),\n",
        "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
        "            ]))\n",
        "        elif method == 'linear':\n",
        "            proj = None\n",
        "        else:\n",
        "            raise ValueError('Unknown method ({})'.format(method))\n",
        "\n",
        "        return proj\n",
        "\n",
        "    def forward_conv(self, x, h, w):\n",
        "        if self.with_cls_token:\n",
        "            cls_token, x = torch.split(x, [1, h*w], 1)\n",
        "\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "\n",
        "        if self.conv_proj_q is not None:\n",
        "            q = self.conv_proj_q(x)\n",
        "        else:\n",
        "            q = rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "        if self.conv_proj_k is not None:\n",
        "            k = self.conv_proj_k(x)\n",
        "        else:\n",
        "            k = rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "        if self.conv_proj_v is not None:\n",
        "            v = self.conv_proj_v(x)\n",
        "        else:\n",
        "            v = rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "        if self.with_cls_token:\n",
        "            q = torch.cat((cls_token, q), dim=1)\n",
        "            k = torch.cat((cls_token, k), dim=1)\n",
        "            v = torch.cat((cls_token, v), dim=1)\n",
        "\n",
        "        return q, k, v\n",
        "\n",
        "    def forward(self, x, h, w):\n",
        "        if (\n",
        "            self.conv_proj_q is not None\n",
        "            or self.conv_proj_k is not None\n",
        "            or self.conv_proj_v is not None\n",
        "        ):\n",
        "            q, k, v = self.forward_conv(x, h, w)\n",
        "\n",
        "        q = rearrange(self.proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "        k = rearrange(self.proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "        v = rearrange(self.proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "\n",
        "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
        "        attn = F.softmax(attn_score, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = torch.einsum('bhlt,bhtv->bhlv', [attn, v])\n",
        "        x = rearrange(x, 'b h t d -> b t (h d)')\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_macs(module, input, output):\n",
        "        # T: num_token\n",
        "        # S: num_token\n",
        "        input = input[0]\n",
        "        flops = 0\n",
        "\n",
        "        _, T, C = input.shape\n",
        "        H = W = int(np.sqrt(T-1)) if module.with_cls_token else int(np.sqrt(T))\n",
        "\n",
        "        H_Q = H / module.stride_q\n",
        "        W_Q = H / module.stride_q\n",
        "        T_Q = H_Q * W_Q + 1 if module.with_cls_token else H_Q * W_Q\n",
        "\n",
        "        H_KV = H / module.stride_kv\n",
        "        W_KV = W / module.stride_kv\n",
        "        T_KV = H_KV * W_KV + 1 if module.with_cls_token else H_KV * W_KV\n",
        "\n",
        "        # C = module.dim\n",
        "        # S = T\n",
        "        # Scaled-dot-product macs\n",
        "        # [B x T x C] x [B x C x T] --> [B x T x S]\n",
        "        # multiplication-addition is counted as 1 because operations can be fused\n",
        "        flops += T_Q * T_KV * module.dim\n",
        "        # [B x T x S] x [B x S x C] --> [B x T x C]\n",
        "        flops += T_Q * module.dim * T_KV\n",
        "\n",
        "        if (\n",
        "            hasattr(module, 'conv_proj_q')\n",
        "            and hasattr(module.conv_proj_q, 'conv')\n",
        "        ):\n",
        "            params = sum(\n",
        "                [\n",
        "                    p.numel()\n",
        "                    for p in module.conv_proj_q.conv.parameters()\n",
        "                ]\n",
        "            )\n",
        "            flops += params * H_Q * W_Q\n",
        "\n",
        "        if (\n",
        "            hasattr(module, 'conv_proj_k')\n",
        "            and hasattr(module.conv_proj_k, 'conv')\n",
        "        ):\n",
        "            params = sum(\n",
        "                [\n",
        "                    p.numel()\n",
        "                    for p in module.conv_proj_k.conv.parameters()\n",
        "                ]\n",
        "            )\n",
        "            flops += params * H_KV * W_KV\n",
        "\n",
        "        if (\n",
        "            hasattr(module, 'conv_proj_v')\n",
        "            and hasattr(module.conv_proj_v, 'conv')\n",
        "        ):\n",
        "            params = sum(\n",
        "                [\n",
        "                    p.numel()\n",
        "                    for p in module.conv_proj_v.conv.parameters()\n",
        "                ]\n",
        "            )\n",
        "            flops += params * H_KV * W_KV\n",
        "\n",
        "        params = sum([p.numel() for p in module.proj_q.parameters()])\n",
        "        flops += params * T_Q\n",
        "        params = sum([p.numel() for p in module.proj_k.parameters()])\n",
        "        flops += params * T_KV\n",
        "        params = sum([p.numel() for p in module.proj_v.parameters()])\n",
        "        flops += params * T_KV\n",
        "        params = sum([p.numel() for p in module.proj.parameters()])\n",
        "        flops += params * T\n",
        "\n",
        "        module.__flops__ += flops\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_in,\n",
        "                 dim_out,\n",
        "                 num_heads,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=False,\n",
        "                 drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 drop_path=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.with_cls_token = kwargs['with_cls_token']\n",
        "\n",
        "        self.norm1 = norm_layer(dim_in)\n",
        "        self.attn = Attention(\n",
        "            dim_in, dim_out, num_heads, qkv_bias, attn_drop, drop,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) \\\n",
        "            if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim_out)\n",
        "\n",
        "        dim_mlp_hidden = int(dim_out * mlp_ratio)\n",
        "        self.mlp = Mlp(\n",
        "            in_features=dim_out,\n",
        "            hidden_features=dim_mlp_hidden,\n",
        "            act_layer=act_layer,\n",
        "            drop=drop\n",
        "        )\n",
        "\n",
        "    def forward(self, x, h, w):\n",
        "        res = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        attn = self.attn(x, h, w)\n",
        "        x = res + self.drop_path(attn)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\" Image to Conv Embedding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 patch_size=7,\n",
        "                 in_chans=3,\n",
        "                 embed_dim=64,\n",
        "                 stride=4,\n",
        "                 padding=2,\n",
        "                 norm_layer=None):\n",
        "        super().__init__()\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=stride,\n",
        "            padding=padding\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 patch_size=16,\n",
        "                 patch_stride=16,\n",
        "                 patch_padding=0,\n",
        "                 in_chans=3,\n",
        "                 embed_dim=768,\n",
        "                 depth=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=False,\n",
        "                 drop_rate=0.,\n",
        "                 attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 init='trunc_norm',\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "\n",
        "        self.rearrage = None\n",
        "\n",
        "        self.patch_embed = ConvEmbed(\n",
        "            # img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            stride=patch_stride,\n",
        "            padding=patch_padding,\n",
        "            embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer\n",
        "        )\n",
        "\n",
        "        with_cls_token = kwargs['with_cls_token']\n",
        "        if with_cls_token:\n",
        "            self.cls_token = nn.Parameter(\n",
        "                torch.zeros(1, 1, embed_dim)\n",
        "            )\n",
        "        else:\n",
        "            self.cls_token = None\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "\n",
        "        blocks = []\n",
        "        for j in range(depth):\n",
        "            blocks.append(\n",
        "                Block(\n",
        "                    dim_in=embed_dim,\n",
        "                    dim_out=embed_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    drop=drop_rate,\n",
        "                    attn_drop=attn_drop_rate,\n",
        "                    drop_path=dpr[j],\n",
        "                    act_layer=act_layer,\n",
        "                    norm_layer=norm_layer,\n",
        "                    **kwargs\n",
        "                )\n",
        "            )\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        if self.cls_token is not None:\n",
        "            trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "        if init == 'xavier':\n",
        "            self.apply(self._init_weights_xavier)\n",
        "        else:\n",
        "            self.apply(self._init_weights_trunc_normal)\n",
        "\n",
        "    def _init_weights_trunc_normal(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            logging.info('=> init weight of Linear from trunc norm')\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                logging.info('=> init bias of Linear to zeros')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def _init_weights_xavier(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            logging.info('=> init weight of Linear from xavier uniform')\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                logging.info('=> init bias of Linear to zeros')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        B, C, H, W = x.size()\n",
        "\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "        cls_tokens = None\n",
        "        if self.cls_token is not None:\n",
        "            # stole cls_tokens impl from Phil Wang, thanks\n",
        "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "            x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x, H, W)\n",
        "\n",
        "        if self.cls_token is not None:\n",
        "            cls_tokens, x = torch.split(x, [1, H*W], 1)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
        "\n",
        "        return x, cls_tokens\n",
        "\n",
        "\n",
        "class ConvolutionalVisionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_chans=3,\n",
        "                 num_classes=1000,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 init='trunc_norm',\n",
        "                 spec=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.num_stages = spec['NUM_STAGES']\n",
        "        for i in range(self.num_stages):\n",
        "            kwargs = {\n",
        "                'patch_size': spec['PATCH_SIZE'][i],\n",
        "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
        "                'patch_padding': spec['PATCH_PADDING'][i],\n",
        "                'embed_dim': spec['DIM_EMBED'][i],\n",
        "                'depth': spec['DEPTH'][i],\n",
        "                'num_heads': spec['NUM_HEADS'][i],\n",
        "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
        "                'qkv_bias': spec['QKV_BIAS'][i],\n",
        "                'drop_rate': spec['DROP_RATE'][i],\n",
        "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
        "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
        "                'with_cls_token': spec['CLS_TOKEN'][i],\n",
        "                'method': spec['QKV_PROJ_METHOD'][i],\n",
        "                'kernel_size': spec['KERNEL_QKV'][i],\n",
        "                'padding_q': spec['PADDING_Q'][i],\n",
        "                'padding_kv': spec['PADDING_KV'][i],\n",
        "                'stride_kv': spec['STRIDE_KV'][i],\n",
        "                'stride_q': spec['STRIDE_Q'][i],\n",
        "            }\n",
        "\n",
        "            stage = VisionTransformer(\n",
        "                in_chans=in_chans,\n",
        "                init=init,\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer,\n",
        "                **kwargs\n",
        "            )\n",
        "            setattr(self, f'stage{i}', stage)\n",
        "\n",
        "            in_chans = spec['DIM_EMBED'][i]\n",
        "\n",
        "        dim_embed = spec['DIM_EMBED'][-1]\n",
        "        self.norm = norm_layer(dim_embed)\n",
        "        self.cls_token = spec['CLS_TOKEN'][-1]\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        trunc_normal_(self.head.weight, std=0.02)\n",
        "\n",
        "    def init_weights(self, pretrained='', pretrained_layers=[], verbose=True):\n",
        "        if os.path.isfile(pretrained):\n",
        "            pretrained_dict = torch.load(pretrained, map_location='cpu')\n",
        "            logging.info(f'=> loading pretrained model {pretrained}')\n",
        "            model_dict = self.state_dict()\n",
        "            pretrained_dict = {\n",
        "                k: v for k, v in pretrained_dict.items()\n",
        "                if k in model_dict.keys()\n",
        "            }\n",
        "            need_init_state_dict = {}\n",
        "            for k, v in pretrained_dict.items():\n",
        "                need_init = (\n",
        "                        k.split('.')[0] in pretrained_layers\n",
        "                        or pretrained_layers[0] is '*'\n",
        "                )\n",
        "                if need_init:\n",
        "                    if verbose:\n",
        "                        logging.info(f'=> init {k} from {pretrained}')\n",
        "                    if 'pos_embed' in k and v.size() != model_dict[k].size():\n",
        "                        size_pretrained = v.size()\n",
        "                        size_new = model_dict[k].size()\n",
        "                        logging.info(\n",
        "                            '=> load_pretrained: resized variant: {} to {}'\n",
        "                            .format(size_pretrained, size_new)\n",
        "                        )\n",
        "\n",
        "                        ntok_new = size_new[1]\n",
        "                        ntok_new -= 1\n",
        "\n",
        "                        posemb_tok, posemb_grid = v[:, :1], v[0, 1:]\n",
        "\n",
        "                        gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                        gs_new = int(np.sqrt(ntok_new))\n",
        "\n",
        "                        logging.info(\n",
        "                            '=> load_pretrained: grid-size from {} to {}'\n",
        "                            .format(gs_old, gs_new)\n",
        "                        )\n",
        "\n",
        "                        posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "                        zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                        posemb_grid = scipy.ndimage.zoom(\n",
        "                            posemb_grid, zoom, order=1\n",
        "                        )\n",
        "                        posemb_grid = posemb_grid.reshape(1, gs_new ** 2, -1)\n",
        "                        v = torch.tensor(\n",
        "                            np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                        )\n",
        "\n",
        "                    need_init_state_dict[k] = v\n",
        "            self.load_state_dict(need_init_state_dict, strict=False)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        layers = set()\n",
        "        for i in range(self.num_stages):\n",
        "            layers.add(f'stage{i}.pos_embed')\n",
        "            layers.add(f'stage{i}.cls_token')\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(self.num_stages):\n",
        "            x, cls_tokens = getattr(self, f'stage{i}')(x)\n",
        "\n",
        "        if self.cls_token:\n",
        "            x = self.norm(cls_tokens)\n",
        "            x = torch.squeeze(x)\n",
        "        else:\n",
        "            x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "            x = self.norm(x)\n",
        "            x = torch.mean(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_cls_model(config, **kwargs):\n",
        "    msvit_spec = config[\"MODEL\"][\"SPEC\"]\n",
        "    msvit = ConvolutionalVisionTransformer(\n",
        "        in_chans=3,\n",
        "        num_classes=10,\n",
        "        act_layer=QuickGELU,\n",
        "        norm_layer=partial(LayerNorm, eps=1e-5),\n",
        "        init=getattr(msvit_spec, 'INIT', 'trunc_norm'),\n",
        "        spec=msvit_spec\n",
        "    )\n",
        "\n",
        "    return msvit"
      ],
      "metadata": {
        "id": "JfwTOJYLIyo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2d1a1d-a429-4eaf-d0f2-b52b3877b8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:573: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "<>:573: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "/tmp/ipython-input-1757211385.py:573: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  or pretrained_layers[0] is '*'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CvT initialization and training"
      ],
      "metadata": {
        "id": "DJn0HH_2vbmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained models configuration\n",
        "\n",
        "with open(\"cvt-13-224x224.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to7-U700UwHC",
        "outputId": "097d7e14-40f8-4f1f-9d14-343738643cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OUTPUT_DIR': 'OUTPUT/', 'WORKERS': 6, 'PRINT_FREQ': 500, 'AMP': {'ENABLED': True}, 'MODEL': {'NAME': 'cls_cvt', 'SPEC': {'INIT': 'trunc_norm', 'NUM_STAGES': 3, 'PATCH_SIZE': [7, 3, 3], 'PATCH_STRIDE': [4, 2, 2], 'PATCH_PADDING': [2, 1, 1], 'DIM_EMBED': [64, 192, 384], 'NUM_HEADS': [1, 3, 6], 'DEPTH': [1, 2, 10], 'MLP_RATIO': [4.0, 4.0, 4.0], 'ATTN_DROP_RATE': [0.0, 0.0, 0.0], 'DROP_RATE': [0.0, 0.0, 0.0], 'DROP_PATH_RATE': [0.0, 0.0, 0.1], 'QKV_BIAS': [True, True, True], 'CLS_TOKEN': [False, False, True], 'POS_EMBED': [False, False, False], 'QKV_PROJ_METHOD': ['dw_bn', 'dw_bn', 'dw_bn'], 'KERNEL_QKV': [3, 3, 3], 'PADDING_KV': [1, 1, 1], 'STRIDE_KV': [2, 2, 2], 'PADDING_Q': [1, 1, 1], 'STRIDE_Q': [1, 1, 1]}}, 'AUG': {'MIXUP_PROB': 1.0, 'MIXUP': 0.8, 'MIXCUT': 1.0, 'TIMM_AUG': {'USE_LOADER': True, 'RE_COUNT': 1, 'RE_MODE': 'pixel', 'RE_SPLIT': False, 'RE_PROB': 0.25, 'AUTO_AUGMENT': 'rand-m9-mstd0.5-inc1', 'HFLIP': 0.5, 'VFLIP': 0.0, 'COLOR_JITTER': 0.4, 'INTERPOLATION': 'bicubic'}}, 'LOSS': {'LABEL_SMOOTHING': 0.1}, 'CUDNN': {'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True}, 'DATASET': {'DATASET': 'imagenet', 'DATA_FORMAT': 'jpg', 'ROOT': 'DATASET/imagenet/', 'TEST_SET': 'val', 'TRAIN_SET': 'train'}, 'TEST': {'BATCH_SIZE_PER_GPU': 32, 'IMAGE_SIZE': [224, 224], 'MODEL_FILE': '', 'INTERPOLATION': 3}, 'TRAIN': {'BATCH_SIZE_PER_GPU': 256, 'LR': 0.00025, 'IMAGE_SIZE': [224, 224], 'BEGIN_EPOCH': 0, 'END_EPOCH': 300, 'LR_SCHEDULER': {'METHOD': 'timm', 'ARGS': {'sched': 'cosine', 'warmup_epochs': 5, 'warmup_lr': 1e-06, 'min_lr': 1e-05, 'cooldown_epochs': 10, 'decay_rate': 0.1}}, 'OPTIMIZER': 'adamW', 'WD': 0.05, 'WITHOUT_WD_LIST': ['bn', 'bias', 'ln'], 'SHUFFLE': True}, 'DEBUG': {'DEBUG': False}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/EuroSAT_RGB/\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = get_cls_model(config).to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/EuroSAT_RGB/CvT-13-224x224-IN-1k.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Load the state dict\n",
        "state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "\n",
        "# Delete heads to make the state dict fit\n",
        "for k in list(state_dict.keys()):\n",
        "    if 'head.' in k:\n",
        "        del state_dict[k]\n",
        "\n",
        "# Apply state dict to the model\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "id": "sS13RvdvYTrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772d5d32-bebd-4f4a-daa7-d53ec344b64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/EuroSAT_RGB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "gt1coT7CvtFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9bc69a-59d5-40d5-d117-eac81b8d84f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19616330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "AEr8j0UGXoKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "id": "h-9XrK5v8Of1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcaf239-e20c-43ed-9ef5-f4b54d124908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Loss: 1.6685\n",
            "Epoch [2/20] - Loss: 1.0223\n",
            "Epoch [3/20] - Loss: 0.7600\n",
            "Epoch [4/20] - Loss: 0.6100\n",
            "Epoch [5/20] - Loss: 0.5118\n",
            "Epoch [6/20] - Loss: 0.4311\n",
            "Epoch [7/20] - Loss: 0.3599\n",
            "Epoch [8/20] - Loss: 0.3073\n",
            "Epoch [9/20] - Loss: 0.2652\n",
            "Epoch [10/20] - Loss: 0.2392\n",
            "Epoch [11/20] - Loss: 0.2026\n",
            "Epoch [12/20] - Loss: 0.1882\n",
            "Epoch [13/20] - Loss: 0.1661\n",
            "Epoch [14/20] - Loss: 0.1501\n",
            "Epoch [15/20] - Loss: 0.1454\n",
            "Epoch [16/20] - Loss: 0.1345\n",
            "Epoch [17/20] - Loss: 0.1233\n",
            "Epoch [18/20] - Loss: 0.1178\n",
            "Epoch [19/20] - Loss: 0.1043\n",
            "Epoch [20/20] - Loss: 0.1029\n",
            "\n",
            "Training completed in: 976.84 seconds (16.28 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "yxpVdolReFnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017732d7-3bba-4d84-c4a1-2152431928ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 96.49%\n"
          ]
        }
      ]
    }
  ]
}